{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in ./venv/lib/python3.11/site-packages (0.10.21)\n",
      "Requirement already satisfied: opencv-python in ./venv/lib/python3.11/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: pyautogui in ./venv/lib/python3.11/site-packages (0.9.54)\n",
      "Requirement already satisfied: tensorflow in ./venv/lib/python3.11/site-packages (2.19.0)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.11/site-packages (1.26.4)\n",
      "Collecting torch\n",
      "  Using cached torch-2.7.1-cp311-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: absl-py in ./venv/lib/python3.11/site-packages (from mediapipe) (2.3.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in ./venv/lib/python3.11/site-packages (from mediapipe) (25.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in ./venv/lib/python3.11/site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in ./venv/lib/python3.11/site-packages (from mediapipe) (0.7.0)\n",
      "Requirement already satisfied: jaxlib in ./venv/lib/python3.11/site-packages (from mediapipe) (0.7.0)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.11/site-packages (from mediapipe) (3.10.3)\n",
      "Requirement already satisfied: opencv-contrib-python in ./venv/lib/python3.11/site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in ./venv/lib/python3.11/site-packages (from mediapipe) (4.25.8)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in ./venv/lib/python3.11/site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: sentencepiece in ./venv/lib/python3.11/site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: pyobjc-core in ./venv/lib/python3.11/site-packages (from pyautogui) (11.1)\n",
      "Requirement already satisfied: pyobjc-framework-quartz in ./venv/lib/python3.11/site-packages (from pyautogui) (11.1)\n",
      "Requirement already satisfied: pymsgbox in ./venv/lib/python3.11/site-packages (from pyautogui) (1.0.9)\n",
      "Requirement already satisfied: pytweening>=1.0.4 in ./venv/lib/python3.11/site-packages (from pyautogui) (1.2.0)\n",
      "Requirement already satisfied: pyscreeze>=0.1.21 in ./venv/lib/python3.11/site-packages (from pyautogui) (1.0.1)\n",
      "Requirement already satisfied: pygetwindow>=0.0.5 in ./venv/lib/python3.11/site-packages (from pyautogui) (0.0.9)\n",
      "Requirement already satisfied: mouseinfo in ./venv/lib/python3.11/site-packages (from pyautogui) (0.1.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./venv/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./venv/lib/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./venv/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./venv/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./venv/lib/python3.11/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.11/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.11/site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.11/site-packages (from tensorflow) (75.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.11/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./venv/lib/python3.11/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./venv/lib/python3.11/site-packages (from tensorflow) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./venv/lib/python3.11/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./venv/lib/python3.11/site-packages (from tensorflow) (1.73.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in ./venv/lib/python3.11/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in ./venv/lib/python3.11/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in ./venv/lib/python3.11/site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in ./venv/lib/python3.11/site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./venv/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./venv/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in ./venv/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in ./venv/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in ./venv/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: pyrect in ./venv/lib/python3.11/site-packages (from pygetwindow>=0.0.5->pyautogui) (0.2.0)\n",
      "Requirement already satisfied: Pillow>=9.3.0 in ./venv/lib/python3.11/site-packages (from pyscreeze>=0.1.21->pyautogui) (11.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
      "Requirement already satisfied: CFFI>=1.0 in ./venv/lib/python3.11/site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./venv/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./venv/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: scipy>=1.12 in ./venv/lib/python3.11/site-packages (from jax->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.11/site-packages (from matplotlib->mediapipe) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.11/site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.11/site-packages (from matplotlib->mediapipe) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.11/site-packages (from matplotlib->mediapipe) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.11/site-packages (from matplotlib->mediapipe) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.11/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: rubicon-objc in ./venv/lib/python3.11/site-packages (from mouseinfo->pyautogui) (0.5.1)\n",
      "Requirement already satisfied: pyperclip in ./venv/lib/python3.11/site-packages (from mouseinfo->pyautogui) (1.9.0)\n",
      "Requirement already satisfied: pyobjc-framework-Cocoa>=11.1 in ./venv/lib/python3.11/site-packages (from pyobjc-framework-quartz->pyautogui) (11.1)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.11/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Using cached torch-2.7.1-cp311-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, jinja2, fsspec, filelock, torch\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.7.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 sympy-1.14.0 torch-2.7.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mediapipe opencv-python pyautogui tensorflow numpy torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAM_W = 1280\n",
    "CAM_H = 720\n",
    "SCREEN_BOX_COORDS = [[676, 585], [1095, 25]]\n",
    "CR_SCREEN_COORDS = (1000, 75)\n",
    "CR_SCREEN_WIDTH = 510\n",
    "CR_SCREEN_HEIGHT = 900\n",
    "ACTION_BOX_WIDTH = 415\n",
    "ACTION_BOX_HEIGHT = 550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default gesture mappings\n",
    "GESTURE_ACTION_MAP_LEFT = {\n",
    "    # Card Selection\n",
    "    \"one_finger_point\": \"select_card_1\",\n",
    "    \"two_finger_point\": \"select_card_2\", \n",
    "    \"three_fingers_up\": \"select_card_3\",\n",
    "    \"four_fingers_up\": \"select_card_4\",\n",
    "    \n",
    "    # Navigation & Reset\n",
    "    \"close_palm\": \"cancel_selection\",\n",
    "    \n",
    "    # Emotes\n",
    "    \"thumbs_down\": \"emote_oops\",\n",
    "    \"thumbs_up\": \"emote_thanks\", \n",
    "    \"ok\": \"emote_well_played\",\n",
    "}\n",
    "\n",
    "GESTURE_ACTION_MAP_RIGHT = {\n",
    "    # Placement & Confirmation\n",
    "    \"one_finger_point\": \"drag_card\",\n",
    "    \"open_palm\": \"deploy\",\n",
    "    \"fist\": \"wait\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui as pag\n",
    "\n",
    "class Controller:\n",
    "    def __init__(self):\n",
    "        self.action_num_l = 0\n",
    "        self.action_num_r = 0\n",
    "        \n",
    "    def route(self, action_num, handedness, **kwargs):\n",
    "        if handedness == 'Right':\n",
    "            if action_num == 2:\n",
    "                pass\n",
    "            elif action_num == self.action_num_r or action_num in [3,4] and self.action_num_r in [3,4]:\n",
    "                return\n",
    "            self.action_num_r = action_num\n",
    "            if self.action_num_r == 0:\n",
    "                self.deploy()\n",
    "            if self.action_num_r == 2:\n",
    "                self.drag(**kwargs)\n",
    "\n",
    "        elif handedness == 'Left':\n",
    "            if action_num == self.action_num_l:\n",
    "                return \n",
    "            self.action_num_l = action_num\n",
    "            if self.action_num_l == 2:\n",
    "                self.select_card(1)\n",
    "            elif self.action_num_l == 5:\n",
    "                self.select_card(2)\n",
    "            elif self.action_num_l == 6:\n",
    "                self.select_card(3)\n",
    "            elif self.action_num_l == 7:\n",
    "                self.select_card(4)\n",
    "            elif self.action_num_l == 4:\n",
    "                self.emote(\"Thumbs up\")\n",
    "            elif self.action_num_l == 8:\n",
    "                self.emote(\"Thumbs down\")\n",
    "            elif self.action_num_l == 3:\n",
    "                self.emote(\"Well played\")\n",
    "        \n",
    "            \n",
    "    def select_card(self, card_num):\n",
    "        # click the right location\n",
    "        # print(f\"Selecting card {card_num}\")\n",
    "        pag.press(str(card_num))\n",
    "        \n",
    "\n",
    "    def deploy(self):\n",
    "        # let go of mouse\n",
    "        print(\"Deploying\")\n",
    "        pag.mouseUp(button='left')\n",
    "    \n",
    "    def emote(self, emote_name):\n",
    "        # click emote menu\n",
    "        pag.press('e')\n",
    "\n",
    "        # pick right emote\n",
    "        if emote_name == 'Thumbs up':\n",
    "            pag.press('w')\n",
    "        elif emote_name == 'Thumbs down':\n",
    "            pag.press('s')\n",
    "        elif emote_name == 'Well played':\n",
    "            pag.press('a')\n",
    "    \n",
    "    def drag(self, **kwargs):\n",
    "        # get coordinates of pinch based on a box displayed on the screen\n",
    "        # mouse down on where pinch is\n",
    "        coords = kwargs['index_tip']\n",
    "        x = coords.x * CAM_W - SCREEN_BOX_COORDS[0][0]\n",
    "        y =  -coords.y * CAM_H + SCREEN_BOX_COORDS[0][1]\n",
    "        x = self._clip(x, 0, 415)\n",
    "        y = self._clip(y, 0, 550)\n",
    "        cr_x = CR_SCREEN_COORDS[0] + x/ACTION_BOX_WIDTH * CR_SCREEN_WIDTH\n",
    "        cr_y = CR_SCREEN_COORDS[1] + CR_SCREEN_HEIGHT - y/ACTION_BOX_HEIGHT * CR_SCREEN_HEIGHT\n",
    "        pag.mouseDown(cr_x, cr_y)\n",
    "    def activate_skill(self):\n",
    "        # click to activate skill\n",
    "        pag.press('e')\n",
    "\n",
    "    def _clip(self, x, low, high):\n",
    "        return min(max(x, low), high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "import copy\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from collections import deque\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "from utils.cvfpscalc import CvFpsCalc\n",
    "from model.keypoint_classifier.keypoint_classifier import KeyPointClassifier\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Argument parsing #################################################################\n",
    "    cap_device = 0\n",
    "    cap_width = 960 * 0.7\n",
    "    cap_height = 540 * 0.7\n",
    "\n",
    "    use_static_image_mode = False\n",
    "    min_detection_confidence = 0.7\n",
    "    min_tracking_confidence = 0.3\n",
    "\n",
    "    use_brect = True\n",
    "\n",
    "    # Camera preparation ###############################################################\n",
    "    cap = cv.VideoCapture(cap_device)\n",
    "    cap.set(cv.CAP_PROP_FRAME_WIDTH, cap_width)\n",
    "    cap.set(cv.CAP_PROP_FRAME_HEIGHT, cap_height)\n",
    "\n",
    "    # BlueStacks connector\n",
    "    controller = Controller()\n",
    "    \n",
    "    # Model load #############################################################\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(\n",
    "        static_image_mode=use_static_image_mode,\n",
    "        max_num_hands=2,\n",
    "        min_detection_confidence=min_detection_confidence,\n",
    "        min_tracking_confidence=min_tracking_confidence,\n",
    "        model_complexity=0\n",
    "    )\n",
    "\n",
    "    keypoint_classifier = KeyPointClassifier()\n",
    "\n",
    "    # Read labels ###########################################################\n",
    "    with open('model/keypoint_classifier/keypoint_classifier_label.csv',\n",
    "              encoding='utf-8-sig') as f:\n",
    "        keypoint_classifier_labels = csv.reader(f)\n",
    "        keypoint_classifier_labels = [\n",
    "            row[0] for row in keypoint_classifier_labels\n",
    "        ]\n",
    "    # with open(\n",
    "    #         'model/point_history_classifier/point_history_classifier_label.csv',\n",
    "    #         encoding='utf-8-sig') as f:\n",
    "    #     point_history_classifier_labels = csv.reader(f)\n",
    "    #     point_history_classifier_labels = [\n",
    "    #         row[0] for row in point_history_classifier_labels\n",
    "    #     ]\n",
    "\n",
    "    # FPS Measurement ########################################################\n",
    "    cvFpsCalc = CvFpsCalc(buffer_len=10)\n",
    "\n",
    "    # Coordinate history #################################################################\n",
    "    history_length = 16\n",
    "    point_history = deque(maxlen=history_length)\n",
    "\n",
    "    # Finger gesture history ################################################\n",
    "    finger_gesture_history = deque(maxlen=history_length)\n",
    "\n",
    "    #  ########################################################################\n",
    "    mode = 0\n",
    "\n",
    "    while True:\n",
    "        fps = cvFpsCalc.get()\n",
    "\n",
    "        # Process Key (ESC: end) #################################################\n",
    "        key = cv.waitKey(10)\n",
    "        if key == 27:  # ESC\n",
    "            print(\"Exiting\")\n",
    "            cap.release()\n",
    "            cv.destroyAllWindows()\n",
    "            return\n",
    "        number, mode = select_mode(key, mode)\n",
    "\n",
    "        # Camera capture #####################################################\n",
    "        ret, image = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        image = cv.flip(image, 1)  # Mirror display\n",
    "        debug_image = copy.deepcopy(image)\n",
    "\n",
    "        # Detection implementation #############################################################\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image)\n",
    "        image.flags.writeable = True\n",
    "\n",
    "        #  ####################################################################\n",
    "        if results.multi_hand_landmarks is not None:\n",
    "            for hand_landmarks, handedness in zip(results.multi_hand_landmarks,\n",
    "                                                  results.multi_handedness):\n",
    "                # Bounding box calculation\n",
    "                brect = calc_bounding_rect(debug_image, hand_landmarks)\n",
    "                # Landmark calculation\n",
    "                landmark_list = calc_landmark_list(debug_image, hand_landmarks)\n",
    "\n",
    "                # Conversion to relative coordinates / normalized coordinates\n",
    "                pre_processed_landmark_list = pre_process_landmark(\n",
    "                    landmark_list)\n",
    "                pre_processed_point_history_list = pre_process_point_history(\n",
    "                    debug_image, point_history)\n",
    "                # Write to the dataset file\n",
    "                logging_csv(number, mode, pre_processed_landmark_list,\n",
    "                            pre_processed_point_history_list)\n",
    "\n",
    "                # Hand sign classification\n",
    "                hand_sign_id = keypoint_classifier(pre_processed_landmark_list)\n",
    "\n",
    "                kwargs = {}\n",
    "                hand_label = handedness.classification[0].label  # \"Left\" or \"Right\"\n",
    "                if hand_label == \"Right\":\n",
    "                    kwargs['index_tip'] = hand_landmarks.landmark[8]  # x, y, z in [0,1] image space\n",
    "\n",
    "                controller.route(hand_sign_id, hand_label, **kwargs)\n",
    "\n",
    "                # Finger gesture classification\n",
    "                # finger_gesture_id = 0\n",
    "                # point_history_len = len(pre_processed_point_history_list)\n",
    "                # if point_history_len == (history_length * 2):\n",
    "                #     finger_gesture_id = point_history_classifier(\n",
    "                #         pre_processed_point_history_list)\n",
    "\n",
    "                # Calculates the gesture IDs in the latest detection\n",
    "                # finger_gesture_history.append(finger_gesture_id)\n",
    "                # most_common_fg_id = Counter(\n",
    "                #     finger_gesture_history).most_common()\n",
    "\n",
    "                # Drawing part\n",
    "                debug_image = draw_bounding_rect(use_brect, debug_image, brect)\n",
    "                debug_image = draw_landmarks(debug_image, landmark_list)\n",
    "                debug_image = draw_info_text(\n",
    "                    debug_image,\n",
    "                    brect,\n",
    "                    handedness,\n",
    "                    keypoint_classifier_labels[hand_sign_id],\n",
    "                    None\n",
    "                    # point_history_classifier_labels[most_common_fg_id[0][0]],\n",
    "                )\n",
    "        else:\n",
    "            point_history.append([0, 0])\n",
    "\n",
    "        debug_image = draw_info(debug_image, fps, mode, number)\n",
    "        cv.rectangle(debug_image, SCREEN_BOX_COORDS[0], SCREEN_BOX_COORDS[1], (255, 0, 0), 5)\n",
    "\n",
    "        # Screen reflection #############################################################\n",
    "        cv.imshow('Hand Gesture Recognition', debug_image)\n",
    "\n",
    "def calc_bounding_rect(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_array = np.empty((0, 2), int)\n",
    "\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "\n",
    "        landmark_point = [np.array((landmark_x, landmark_y))]\n",
    "\n",
    "        landmark_array = np.append(landmark_array, landmark_point, axis=0)\n",
    "\n",
    "    x, y, w, h = cv.boundingRect(landmark_array)\n",
    "\n",
    "    return [x, y, x + w, y + h]\n",
    "\n",
    "\n",
    "def calc_landmark_list(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_point = []\n",
    "\n",
    "    # Keypoint\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        # landmark_z = landmark.z\n",
    "\n",
    "        landmark_point.append([landmark_x, landmark_y])\n",
    "\n",
    "    return landmark_point\n",
    "\n",
    "\n",
    "def pre_process_landmark(landmark_list):\n",
    "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "\n",
    "    # Convert to relative coordinates\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, landmark_point in enumerate(temp_landmark_list):\n",
    "        if index == 0:\n",
    "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
    "\n",
    "        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
    "        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
    "\n",
    "    # Convert to a one-dimensional list\n",
    "    temp_landmark_list = list(\n",
    "        itertools.chain.from_iterable(temp_landmark_list))\n",
    "\n",
    "    # Normalization\n",
    "    max_value = max(list(map(abs, temp_landmark_list)))\n",
    "\n",
    "    def normalize_(n):\n",
    "        return n / max_value\n",
    "\n",
    "    temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
    "\n",
    "    return temp_landmark_list\n",
    "\n",
    "\n",
    "def pre_process_point_history(image, point_history):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    temp_point_history = copy.deepcopy(point_history)\n",
    "\n",
    "    # Convert to relative coordinates\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, point in enumerate(temp_point_history):\n",
    "        if index == 0:\n",
    "            base_x, base_y = point[0], point[1]\n",
    "\n",
    "        temp_point_history[index][0] = (temp_point_history[index][0] -\n",
    "                                        base_x) / image_width\n",
    "        temp_point_history[index][1] = (temp_point_history[index][1] -\n",
    "                                        base_y) / image_height\n",
    "\n",
    "    # Convert to a one-dimensional list\n",
    "    temp_point_history = list(\n",
    "        itertools.chain.from_iterable(temp_point_history))\n",
    "\n",
    "    return temp_point_history\n",
    "\n",
    "\n",
    "def logging_csv(number, mode, landmark_list, point_history_list):\n",
    "    if mode == 0:\n",
    "        pass\n",
    "    if mode == 1 and (0 <= number <= 9):\n",
    "        csv_path = 'model/keypoint_classifier/keypoint.csv'\n",
    "        with open(csv_path, 'a', newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([number, *landmark_list])\n",
    "    if mode == 2 and (0 <= number <= 9):\n",
    "        csv_path = 'model/point_history_classifier/point_history.csv'\n",
    "        with open(csv_path, 'a', newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([number, *point_history_list])\n",
    "    return\n",
    "\n",
    "\n",
    "def draw_landmarks(image, landmark_point):\n",
    "    if len(landmark_point) > 0:\n",
    "        # Thumb\n",
    "        cv.line(image, tuple(landmark_point[2]), tuple(landmark_point[3]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(landmark_point[2]), tuple(landmark_point[3]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv.line(image, tuple(landmark_point[3]), tuple(landmark_point[4]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(landmark_point[3]), tuple(landmark_point[4]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Index finger\n",
    "        cv.line(image, tuple(landmark_point[5]), tuple(landmark_point[6]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(landmark_point[5]), tuple(landmark_point[6]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv.line(image, tuple(landmark_point[6]), tuple(landmark_point[7]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(landmark_point[6]), tuple(landmark_point[7]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv.line(image, tuple(landmark_point[7]), tuple(landmark_point[8]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(landmark_point[7]), tuple(landmark_point[8]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Middle finger\n",
    "        cv.line(image, tuple(landmark_point[9]), tuple(landmark_point[10]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(landmark_point[9]), tuple(landmark_point[10]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv.line(image, tuple(landmark_point[10]), tuple(landmark_point[11]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(landmark_point[10]), tuple(landmark_point[11]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv.line(image, tuple(landmark_point[11]), tuple(landmark_point[12]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(landmark_point[11]), tuple(landmark_point[12]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Ring finger\n",
    "        cv.line(image, tuple(landmark_point[13]), tuple(landmark_point[14]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(landmark_point[13]), tuple(landmark_point[14]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv.line(image, tuple(landmark_point[14]), tuple(landmark_point[15]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(landmark_point[14]), tuple(landmark_point[15]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv.line(image, tuple(landmark_point[15]), tuple(landmark_point[16]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(landmark_point[15]), tuple(landmark_point[16]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Little finger\n",
    "        cv.line(image, tuple(landmark_point[17]), tuple(landmark_point[18]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(landmark_point[17]), tuple(landmark_point[18]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv.line(image, tuple(landmark_point[18]), tuple(landmark_point[19]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(landmark_point[18]), tuple(landmark_point[19]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv.line(image, tuple(landmark_point[19]), tuple(landmark_point[20]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(landmark_point[19]), tuple(landmark_point[20]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Palm\n",
    "        cv.line(image, tuple(landmark_point[0]), tuple(landmark_point[1]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(landmark_point[0]), tuple(landmark_point[1]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv.line(image, tuple(landmark_point[1]), tuple(landmark_point[2]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(landmark_point[1]), tuple(landmark_point[2]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv.line(image, tuple(landmark_point[2]), tuple(landmark_point[5]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(landmark_point[2]), tuple(landmark_point[5]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv.line(image, tuple(landmark_point[5]), tuple(landmark_point[9]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(landmark_point[5]), tuple(landmark_point[9]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv.line(image, tuple(landmark_point[9]), tuple(landmark_point[13]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(landmark_point[9]), tuple(landmark_point[13]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv.line(image, tuple(landmark_point[13]), tuple(landmark_point[17]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(landmark_point[13]), tuple(landmark_point[17]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv.line(image, tuple(landmark_point[17]), tuple(landmark_point[0]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv.line(image, tuple(landmark_point[17]), tuple(landmark_point[0]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "    # Key Points\n",
    "    for index, landmark in enumerate(landmark_point):\n",
    "        if index == 0:  # 手首1\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 1:  # 手首2\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 2:  # 親指：付け根\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 3:  # 親指：第1関節\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 4:  # 親指：指先\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "        if index == 5:  # 人差指：付け根\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 6:  # 人差指：第2関節\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 7:  # 人差指：第1関節\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 8:  # 人差指：指先\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "        if index == 9:  # 中指：付け根\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 10:  # 中指：第2関節\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 11:  # 中指：第1関節\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 12:  # 中指：指先\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "        if index == 13:  # 薬指：付け根\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 14:  # 薬指：第2関節\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 15:  # 薬指：第1関節\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 16:  # 薬指：指先\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "        if index == 17:  # 小指：付け根\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 18:  # 小指：第2関節\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 19:  # 小指：第1関節\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 20:  # 小指：指先\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_bounding_rect(use_brect, image, brect):\n",
    "    if use_brect:\n",
    "        # Outer rectangle\n",
    "        cv.rectangle(image, (brect[0], brect[1]), (brect[2], brect[3]),\n",
    "                     (0, 0, 0), 1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_info_text(image, brect, handedness, hand_sign_text,\n",
    "                   finger_gesture_text):\n",
    "    cv.rectangle(image, (brect[0], brect[1]), (brect[2], brect[1] - 22),\n",
    "                 (0, 0, 0), -1)\n",
    "\n",
    "    info_text = handedness.classification[0].label[0:]\n",
    "    if hand_sign_text != \"\":\n",
    "        info_text = info_text + ':' + hand_sign_text\n",
    "    cv.putText(image, info_text, (brect[0] + 5, brect[1] - 4),\n",
    "               cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv.LINE_AA)\n",
    "\n",
    "    if finger_gesture_text != \"\" and finger_gesture_text is not None:\n",
    "        cv.putText(image, \"Finger Gesture:\" + finger_gesture_text, (10, 60),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 0), 4, cv.LINE_AA)\n",
    "        cv.putText(image, \"Finger Gesture:\" + finger_gesture_text, (10, 60),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2,\n",
    "                   cv.LINE_AA)\n",
    "\n",
    "    return image\n",
    "\n",
    "def draw_info(image, fps, mode, number):\n",
    "    cv.putText(image, \"FPS:\" + str(fps), (10, 30), cv.FONT_HERSHEY_SIMPLEX,\n",
    "               1.0, (0, 0, 0), 4, cv.LINE_AA)\n",
    "    cv.putText(image, \"FPS:\" + str(fps), (10, 30), cv.FONT_HERSHEY_SIMPLEX,\n",
    "               1.0, (255, 255, 255), 2, cv.LINE_AA)\n",
    "\n",
    "    mode_string = ['Logging Key Point', 'Logging Point History']\n",
    "    if 1 <= mode <= 2:\n",
    "        cv.putText(image, \"MODE:\" + mode_string[mode - 1], (10, 90),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1,\n",
    "                   cv.LINE_AA)\n",
    "        if 0 <= number <= 9:\n",
    "            cv.putText(image, \"NUM:\" + str(number), (10, 110),\n",
    "                       cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1,\n",
    "                       cv.LINE_AA)\n",
    "    return image\n",
    "\n",
    "def select_mode(key, mode):\n",
    "    number = -1\n",
    "    if 48 <= key <= 57:  # 0 ~ 9\n",
    "        number = key - 48\n",
    "    if key == 110:  # n\n",
    "        mode = 0\n",
    "    if key == 107:  # k\n",
    "        mode = 1\n",
    "    if key == 104:  # h\n",
    "        mode = 2\n",
    "    return number, mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 19:59:16.636 Python[39759:36994782] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1753271958.570195 36994782 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M3\n",
      "/Users/marcolee/Desktop/pathpulse/projects/gesture-control-cr/venv/lib/python3.11/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1753271958.601818 36995276 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753271958.605249 36995276 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2025-07-23 19:59:21.102 Python[39759:36994782] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-07-23 19:59:21.102 Python[39759:36994782] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n",
      "W0000 00:00:1753271977.814476 36995273 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying\n",
      "Deploying\n",
      "Deploying\n",
      "Deploying\n",
      "Deploying\n",
      "Deploying\n",
      "Deploying\n",
      "Deploying\n",
      "Deploying\n",
      "Deploying\n",
      "Deploying\n",
      "Deploying\n",
      "Deploying\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     97\u001b[39m image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n\u001b[32m     99\u001b[39m image.flags.writeable = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m results = \u001b[43mhands\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m image.flags.writeable = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m#  ####################################################################\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/pathpulse/projects/gesture-control-cr/venv/lib/python3.11/site-packages/mediapipe/python/solutions/hands.py:153\u001b[39m, in \u001b[36mHands.process\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np.ndarray) -> NamedTuple:\n\u001b[32m    133\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[32m    134\u001b[39m \n\u001b[32m    135\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    150\u001b[39m \u001b[33;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/pathpulse/projects/gesture-control-cr/venv/lib/python3.11/site-packages/mediapipe/python/solution_base.py:340\u001b[39m, in \u001b[36mSolutionBase.process\u001b[39m\u001b[34m(self, input_data)\u001b[39m\n\u001b[32m    334\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    335\u001b[39m     \u001b[38;5;28mself\u001b[39m._graph.add_packet_to_input_stream(\n\u001b[32m    336\u001b[39m         stream=stream_name,\n\u001b[32m    337\u001b[39m         packet=\u001b[38;5;28mself\u001b[39m._make_packet(input_stream_type,\n\u001b[32m    338\u001b[39m                                  data).at(\u001b[38;5;28mself\u001b[39m._simulated_timestamp))\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
